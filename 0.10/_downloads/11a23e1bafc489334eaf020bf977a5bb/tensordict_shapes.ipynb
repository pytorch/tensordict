{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Manipulating the shape of a TensorDict\n**Author**: [Tom Begley](https://github.com/tcbegley)\n\nIn this tutorial you will learn how to manipulate the shape of a :class:`~.TensorDict`\nand its contents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we create a :class:`~.TensorDict` we specify a ``batch_size``, which must agree\nwith the leading dimensions of all entries in the :class:`~.TensorDict`. Since we have\na guarantee that all entries share those dimensions in common, :class:`~.TensorDict`\nis able to expose a number of methods with which we can manipulate the shape of the\n:class:`~.TensorDict` and its contents.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom tensordict.tensordict import TensorDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Indexing a ``TensorDict``\n\nSince the batch dimensions are guaranteed to exist on all entries, we can index them\nas we please, and each entry of the :class:`~.TensorDict` will be indexed in the same\nway.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "a = torch.rand(3, 4)\nb = torch.rand(3, 4, 5)\ntensordict = TensorDict({\"a\": a, \"b\": b}, batch_size=[3, 4])\n\nindexed_tensordict = tensordict[:2, 1]\nassert indexed_tensordict[\"a\"].shape == torch.Size([2])\nassert indexed_tensordict[\"b\"].shape == torch.Size([2, 5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reshaping a ``TensorDict``\n\n:meth:`TensorDict.reshape <tensordict.TensorDict.reshape>` works just like\n:meth:`torch.Tensor.reshape`. It applies to all of the contents of the\n:class:`~.TensorDict` along the batch dimensions - note the shape of ``b`` in the\nexample below. It also updates the ``batch_size`` attribute.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reshaped_tensordict = tensordict.reshape(-1)\nassert reshaped_tensordict.batch_size == torch.Size([12])\nassert reshaped_tensordict[\"a\"].shape == torch.Size([12])\nassert reshaped_tensordict[\"b\"].shape == torch.Size([12, 5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Splitting a ``TensorDict``\n\n:meth:`TensorDict.split <tensordict.TensorDict.split>` is similar to\n:meth:`torch.Tensor.split`. It splits the :class:`~.TensorDict` into chunks. Each\nchunk is a :class:`~.TensorDict` with the same structure as the original one, but\nwhose entries are views of the corresponding entries in the original\n:class:`~.TensorDict`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "chunks = tensordict.split([3, 1], dim=1)\nassert chunks[0].batch_size == torch.Size([3, 3])\nassert chunks[1].batch_size == torch.Size([3, 1])\ntorch.testing.assert_close(chunks[0][\"a\"], tensordict[\"a\"][:, :-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Whenever a function or method accepts a ``dim`` argument, negative dimensions are\n   interpreted relative to the ``batch_size`` of the :class:`~.TensorDict` that the\n   function or method is called on. In particular, if there are nested\n   :class:`~.TensorDict` values with different batch sizes, the negative dimension is\n   always interpreted relative to the batch dimensions of the root.\n\n      >>> tensordict = TensorDict(\n      ...     {\n      ...         \"a\": torch.rand(3, 4),\n      ...         \"nested\": TensorDict({\"b\": torch.rand(3, 4, 5)}, [3, 4, 5])\n      ...     },\n      ...     [3, 4],\n      ... )\n      >>> # dim = -2 will be interpreted as the first dimension throughout, as the root\n      >>> # TensorDict has 2 batch dimensions, even though the nested TensorDict has 3\n      >>> chunks = tensordict.split([2, 1], dim=-2)\n      >>> assert chunks[0].batch_size == torch.Size([2, 4])\n      >>> assert chunks[0][\"nested\"].batch_size == torch.Size([2, 4, 5])\n\n   As you can see from this example, the\n   :meth:`TensorDict.split <tensordict.TensorDict.split>` method behaves exactly as\n   though we had replaced ``dim=-2`` with ``dim=tensordict.batch_dims - 2`` before\n   calling.</p></div>\n\n## Unbind\n:meth:`TensorDict.unbind <tensordict.TensorDict.unbind>` is similar to\n:meth:`torch.Tensor.unbind`, and conceptually similar to\n:meth:`TensorDict.split <tensordict.TensorDict.split>`. It removes the specified\ndimension and returns a ``tuple`` of all slices along that dimension.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "slices = tensordict.unbind(dim=1)\nassert len(slices) == 4\nassert all(s.batch_size == torch.Size([3]) for s in slices)\ntorch.testing.assert_close(slices[0][\"a\"], tensordict[\"a\"][:, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stacking and concatenating\n\n:class:`~.TensorDict` can be used in conjunction with ``torch.cat`` and ``torch.stack``.\n\n### Stacking ``TensorDict``\nStacking can done lazily or contiguously. A lazy stack is just a list of tensordicts\npresented as a stack of tensordicts. It allows users to carry a bag of tensordicts\nwith different content shape, device or key sets. Another advantage is that\nthe stack operation can be expensive, and if only a small subset of keys is required,\na lazy stack will be much faster than a proper stack.\nIt relies on the :class:`~tensordict.LazyStackedTensorDict` class.\nIn this case, values will only be stacked on-demand when they are accessed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensordict import LazyStackedTensorDict\n\ncloned_tensordict = tensordict.clone()\nstacked_tensordict = LazyStackedTensorDict.lazy_stack(\n    [tensordict, cloned_tensordict], dim=0\n)\nprint(stacked_tensordict)\n\n# Previously, torch.stack was always returning a lazy stack. For consistency with\n# the regular PyTorch API, this behaviour will soon be adapted to deliver only\n# dense tensordicts. To control which behaviour you are relying on, you can use\n# the :func:`~tensordict.utils.set_lazy_legacy` decorator/context manager:\n\nfrom tensordict.utils import set_lazy_legacy\n\nwith set_lazy_legacy(True):  # old behaviour\n    lazy_stack = torch.stack([tensordict, cloned_tensordict])\nassert isinstance(lazy_stack, LazyStackedTensorDict)\n\nwith set_lazy_legacy(False):  # new behaviour\n    dense_stack = torch.stack([tensordict, cloned_tensordict])\nassert isinstance(dense_stack, TensorDict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we index a :class:`~.LazyStackedTensorDict` along the stacking dimension we recover\nthe original :class:`~.TensorDict`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert stacked_tensordict[0] is tensordict\nassert stacked_tensordict[1] is cloned_tensordict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accessing a key in the :class:`~.LazyStackedTensorDict` results in those values being\nstacked. If the key corresponds to a nested :class:`~.TensorDict` then we will recover\nanother :class:`~.LazyStackedTensorDict`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert stacked_tensordict[\"a\"].shape == torch.Size([2, 3, 4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Since values are stacked on-demand, accessing an item multiple times will mean it\n   gets stacked multiple times, which is inefficient. If you need to access a value\n   in the stacked :class:`~.TensorDict` more than once, you may want to consider\n   converting the :class:`LazyStackedTensorDict` to a contiguous\n   :class:`~.TensorDict`, which can be done with the\n   :meth:`LazyStackedTensorDict.to_tensordict <tensordict.LazyStackedTensorDict.to_tensordict>`\n   or :meth:`LazyStackedTensorDict.contiguous <tensordict.LazyStackedTensorDict.contiguous>`\n   methods.\n\n      >>> assert isinstance(stacked_tensordict.contiguous(), TensorDict)\n      >>> assert isinstance(stacked_tensordict.contiguous(), TensorDict)\n\n   After calling either of these methods, we will have a regular :class:`TensorDict`\n   containing the stacked values, and no additional computation is performed when\n   values are accessed.</p></div>\n\n### Concatenating ``TensorDict``\nConcatenation is not done lazily, instead calling :func:`torch.cat` on a list of\n:class:`~.TensorDict` instances simply returns a :class:`~.TensorDict` whose entries\nare the concatenated entries of the elements of the list.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "concatenated_tensordict = torch.cat([tensordict, cloned_tensordict], dim=0)\nassert isinstance(concatenated_tensordict, TensorDict)\nassert concatenated_tensordict.batch_size == torch.Size([6, 4])\nassert concatenated_tensordict[\"b\"].shape == torch.Size([6, 4, 5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Expanding ``TensorDict``\nWe can expand all of the entries of a :class:`~.TensorDict` using\n:meth:`TensorDict.expand <tensordict.TensorDict.expand>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "exp_tensordict = tensordict.expand(2, *tensordict.batch_size)\nassert exp_tensordict.batch_size == torch.Size([2, 3, 4])\ntorch.testing.assert_close(exp_tensordict[\"a\"][0], exp_tensordict[\"a\"][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Squeezing and Unsqueezing ``TensorDict``\nWe can squeeze or unsqueeze the contents of a :class:`~.TensorDict` with the\n:meth:`~tensordict.TensorDictBase.squeeze` and\n:meth:`~tensordict.TensorDictBase.unsqueeze` methods.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict({\"a\": torch.rand(3, 1, 4)}, [3, 1, 4])\nsqueezed_tensordict = tensordict.squeeze()\nassert squeezed_tensordict[\"a\"].shape == torch.Size([3, 4])\nprint(squeezed_tensordict, end=\"\\n\\n\")\n\nunsqueezed_tensordict = tensordict.unsqueeze(-1)\nassert unsqueezed_tensordict[\"a\"].shape == torch.Size([3, 1, 4, 1])\nprint(unsqueezed_tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Until now, operations like :meth:`~tensordict.TensorDictBase.unsqueeze`,\n   :meth:`~tensordict.TensorDictBase.squeeze`, :meth:`~tensordict.TensorDictBase.view`,\n   :meth:`~tensordict.TensorDictBase.permute`, :meth:`~tensordict.TensorDictBase.transpose`\n   were all returning a lazy version of these operations (ie, a container where the original\n   tensordict was stored and where the operations was applied every time a key was accessed).\n   This behaviour will be deprecated in the future and can be already controlled via the\n   :func:`~tensordict.utils.set_lazy_legacy` function:\n\n      >>> with set_lazy_legacy(True):\n      ...     lazy_unsqueeze = tensordict.unsqueeze(0)\n      >>> with set_lazy_legacy(False):\n      ...     dense_unsqueeze = tensordict.unsqueeze(0)</p></div>\n\nBear in mind that as ever, these methods apply only to the batch dimensions. Any non\nbatch dimensions of the entries will be unaffected\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict({\"a\": torch.rand(3, 1, 1, 4)}, [3, 1])\nsqueezed_tensordict = tensordict.squeeze()\n# only one of the singleton dimensions is dropped as the other\n# is not a batch dimension\nassert squeezed_tensordict[\"a\"].shape == torch.Size([3, 1, 4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Viewing a TensorDict\n:class:`~.TensorDict` also supports ``view``. This creates a ``_ViewedTensorDict``\nwhich lazily creates views on its contents when they are accessed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict({\"a\": torch.arange(12)}, [12])\n# no views are created at this step\nviewed_tensordict = tensordict.view((2, 3, 2))\n\n# the view of \"a\" is created on-demand when we access it\nassert viewed_tensordict[\"a\"].shape == torch.Size([2, 3, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Permuting batch dimensions\nThe :meth:`TensorDict.permute <tensordict.TensorDict.permute>` method can be used to\npermute the batch dimensions much like :func:`torch.permute`. Non batch dimensions are\nleft untouched.\n\nThis operation is lazy, so batch dimensions are only permuted when we try to access\nthe entries. As ever, if you are likely to need to access a particular entry multiple\ntimes, consider converting to a :class:`~.TensorDict`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict({\"a\": torch.rand(3, 4), \"b\": torch.rand(3, 4, 5)}, [3, 4])\n# swap the batch dimensions\npermuted_tensordict = tensordict.permute([1, 0])\n\nassert permuted_tensordict[\"a\"].shape == torch.Size([4, 3])\nassert permuted_tensordict[\"b\"].shape == torch.Size([4, 3, 5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using tensordicts as decorators\n\nFor a bunch of reversible operations, tensordicts can be used as decorators.\nThese operations include :meth:`~tensordict.TensorDictBase.to_module` for functional\ncalls, :meth:`~tensordict.TensorDictBase.unlock_` and :meth:`~tensordict.TensorDictBase.lock_`\nor shape operations such as :meth:`~tensordict.TensorDictBase.view`, :meth:`~tensordict.TensorDictBase.permute`\n:meth:`~tensordict.TensorDictBase.transpose`, :meth:`~tensordict.TensorDictBase.squeeze` and\n:meth:`~tensordict.TensorDictBase.unsqueeze`.\nHere is a quick example with the ``transpose`` function:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict({\"a\": torch.rand(3, 4), \"b\": torch.rand(3, 4, 5)}, [3, 4])\n\nwith tensordict.transpose(1, 0) as tdt:\n    tdt.set(\"c\", torch.ones(4, 3))  # we have permuted the dims\n\n# the ``\"c\"`` entry is now in the tensordict we used as decorator:\n#\n\nassert (tensordict.get(\"c\") == 1).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gathering values in ``TensorDict``\nThe :meth:`TensorDict.gather <tensordict.TensorDict.gather>` method can be used to\nindex along the batch dimensions and gather the results into a single dimension much\nlike :func:`torch.gather`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "index = torch.randint(4, (3, 4))\ngathered_tensordict = tensordict.gather(dim=1, index=index)\nprint(\"index:\\n\", index, end=\"\\n\\n\")\nprint(\"tensordict['a']:\\n\", tensordict[\"a\"], end=\"\\n\\n\")\nprint(\"gathered_tensordict['a']:\\n\", gathered_tensordict[\"a\"], end=\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}