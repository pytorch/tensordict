{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# TensorDictModule\n\n\n**Author**: [Nicolas Dufour](https://github.com/nicolas-dufour), [Vincent Moens](https://github.com/vmoens)\n\nIn this tutorial you will learn how to use :class:`~.TensorDictModule` and\n:class:`~.TensorDictSequential` to create generic and reusable modules that can accept\n:class:`~.TensorDict` as input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For a convenient usage of the :class:`~.TensorDict` class with :class:`~torch.nn.Module`,\n:mod:`tensordict` provides an interface between the two named :class:`~tensordict.nn.TensorDictModule`.\n\nThe :class:`~tensordict.nn.TensorDictModule` class is an :class:`~torch.nn.Module` that takes a\n:class:`~tensordict.TensorDict` as input when called. It will read a sequence of input keys, pass them to the wrapped\nmodule or function as input, and write the outputs in the same tensordict after completing the execution.\n\nIt is up to the user to define the keys to be read as input and output.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nfrom tensordict import TensorDict\nfrom tensordict.nn import TensorDictModule, TensorDictSequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple example: coding a recurrent layer\n\nThe simplest usage of :class:`~tensordict.nn.TensorDictModule` is exemplified below.\nIf at first it may look like using this class introduces an unwated level of complexity, we will see\nlater on that this API enables users to programatically concatenate modules together, cache values\nin between modules or programmatically build one.\nOne of the simplest examples of this is a recurrent module in an architecture like ResNet, where the input of the\nmodule is cached and added to the output of a tiny multi-layered perceptron (MLP).\n\nTo start, let's first consider we you would chunk an MLP, and code it using :mod:`tensordict.nn`.\nThe first layer of the stack would presumably be a :class:`~torch.nn.Linear` layer, taking an entry as input\n(let us name it `x`) and outputting another entry (which we will name `y`).\n\nTo feed to our module, we have a :class:`~tensordict.TensorDict` instance with a single entry,\n``\"x\"``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict(\n    x=torch.randn(5, 3),\n    batch_size=[5],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we build our simple module using :class:`tensordict.nn.TensorDictModule`. By default, this class writes in the\ninput tensordict in-place (meaning that entries are written in the same tensordict as the input, not that entries\nare overwritten in-place!), such that we don't need to explicitly indicate what the output is:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "linear0 = TensorDictModule(nn.Linear(3, 128), in_keys=[\"x\"], out_keys=[\"linear0\"])\nlinear0(tensordict)\n\nassert \"linear0\" in tensordict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the module outputs multiple tensors (or tensordicts!) their entries must be passed to\n:class:`~tensordict.nn.TensorDictModule` in the right order.\n\n### Support for Callables\n\nWhen designing a model, it often happens that you want to incorporate an arbitrary non-parametric function into\nthe network. For instance, you may wish to permute the dimensions of an image when it is passed to a convolutional network\nor a vision transformer, or divide the values by 255.\nThere are several ways to do this: you could use a `forward_hook`, for example, or design a new\n:class:`~torch.nn.Module` that performs this operation.\n\n:class:`~tensordict.nn.TensorDictModule` works with any callable, not just modules, which makes it easy to\nincorporate arbitrary functions into a module. For instance, let's see how we can integrate the ``relu`` activation\nfunction without using the :class:`~torch.nn.ReLU` module:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "relu0 = TensorDictModule(torch.relu, in_keys=[\"linear0\"], out_keys=[\"relu0\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stacking modules\n\nOur MLP isn't made of a single layer, so we now need to add another layer to it.\nThis layer will be an activation function, for instance :class:`~torch.nn.ReLU`.\nWe can stack this module and the previous one using :class:`~tensordict.nn.TensorDictSequential`.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Here comes the true power of ``tensordict.nn``: unlike :class:`~torch.nn.Sequential`,\n  :class:`~tensordict.nn.TensorDictSequential` will keep in memory all the previous inputs and outputs\n  (with the possibility to filter them out afterwards), making it easy to have complex network structures\n  built on-the-fly and programmatically.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "block0 = TensorDictSequential(linear0, relu0)\n\nblock0(tensordict)\nassert \"linear0\" in tensordict\nassert \"relu0\" in tensordict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can repeat this logic to get a full MLP:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "linear1 = TensorDictModule(nn.Linear(128, 128), in_keys=[\"relu0\"], out_keys=[\"linear1\"])\nrelu1 = TensorDictModule(nn.ReLU(), in_keys=[\"linear1\"], out_keys=[\"relu1\"])\nlinear2 = TensorDictModule(nn.Linear(128, 3), in_keys=[\"relu1\"], out_keys=[\"linear2\"])\nblock1 = TensorDictSequential(linear1, relu1, linear2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multiple input keys\n\nThe last step of the residual network is to add the input to the output of the last linear layer.\nNo need to write a special :class:`~torch.nn.Module` subclass for this! :class:`~tensordict.nn.TensorDictModule`\ncan be used to wrap simple functions too:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "residual = TensorDictModule(\n    lambda x, y: x + y, in_keys=[\"x\", \"linear2\"], out_keys=[\"y\"]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we can now put together ``block0``, ``block1`` and ``residual`` for a fully fleshed residual block:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "block = TensorDictSequential(block0, block1, residual)\nblock(tensordict)\nassert \"y\" in tensordict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A genuine concern may be the accumulation of entries in the tensordict used as input: in some cases (e.g., when\ngradients are required) intermediate values may be cached anyway, but this isn't always the case and it can be useful\nto let the garbage collector know that some entries can be discarded. :class:`tensordict.nn.TensorDictModuleBase` and\nits subclasses (including :class:`tensordict.nn.TensorDictModule` and :class:`tensordict.nn.TensorDictSequential`)\nhave the option of seeing their output keys filtered after execution. To do this, just call the\n:class:`tensordict.nn.TensorDictModuleBase.select_out_keys` method. This will update the module in-place and all the\nunwanted entries will be discarded:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "block.select_out_keys(\"y\")\n\ntensordict = TensorDict(x=torch.randn(1, 3), batch_size=[1])\nblock(tensordict)\nassert \"y\" in tensordict\n\nassert \"linear1\" not in tensordict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, the input keys are preserved:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert \"x\" in tensordict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a side note, ``selected_out_keys`` may also be passed to :class:`tensordict.nn.TensorDictSequential` to avoid\ncalling this method separately.\n\n## Using `TensorDictModule` without tensordict\n\nThe opportunity offered by :class:`tensordict.nn.TensorDictSequential` to build complex architectures on-the-go\ndoes not mean that one necessarily has to switch to tensordict to represent the data. Thanks to\n:class:`~tensordict.nn.dispatch`, modules from `tensordict.nn` support arguments and keyword arguments that match the\nentry names too:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.randn(1, 3)\ny = block(x=x)\nassert isinstance(y, torch.Tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Under the hood, :class:`~tensordict.nn.dispatch` rebuilds a tensordict, runs the module and then deconstructs it.\nThis may cause some overhead but, as we will see just after, there is a solution to get rid of this.\n\n## Runtime\n\n:class:`tensordict.nn.TensorDictModule` and :class:`tensordict.nn.TensorDictSequential` do incur some overhead when\nexecuted, as they need to read and write from a tensordict. However, we can greatly reduce this overhead by using\n:func:`~torch.compile`. For this, let us compare the three versions of this code with and without compile:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear0 = nn.Linear(3, 128)\n        self.relu0 = nn.ReLU()\n        self.linear1 = nn.Linear(128, 128)\n        self.relu1 = nn.ReLU()\n        self.linear2 = nn.Linear(128, 3)\n\n    def forward(self, x):\n        y = self.linear0(x)\n        y = self.relu0(y)\n        y = self.linear1(y)\n        y = self.relu1(y)\n        return self.linear2(y) + x\n\n\nprint(\"Without compile\")\nx = torch.randn(256, 3)\nblock_notd = ResidualBlock()\nblock_tdm = TensorDictModule(block_notd, in_keys=[\"x\"], out_keys=[\"y\"])\nblock_tds = block\n\nfrom torch.utils.benchmark import Timer\n\nprint(\n    f\"Regular: {Timer('block_notd(x=x)', globals=globals()).adaptive_autorange().median * 1_000_000: 4.4f} us\"\n)\nprint(\n    f\"TDM: {Timer('block_tdm(x=x)', globals=globals()).adaptive_autorange().median * 1_000_000: 4.4f} us\"\n)\nprint(\n    f\"Sequential: {Timer('block_tds(x=x)', globals=globals()).adaptive_autorange().median * 1_000_000: 4.4f} us\"\n)\n\nprint(\"Compiled versions\")\nblock_notd_c = torch.compile(block_notd, mode=\"reduce-overhead\")\nfor _ in range(5):  # warmup\n    block_notd_c(x)\nprint(\n    f\"Compiled regular: {Timer('block_notd_c(x=x)', globals=globals()).adaptive_autorange().median * 1_000_000: 4.4f} us\"\n)\nblock_tdm_c = torch.compile(block_tdm, mode=\"reduce-overhead\")\nfor _ in range(5):  # warmup\n    block_tdm_c(x=x)\nprint(\n    f\"Compiled TDM: {Timer('block_tdm_c(x=x)', globals=globals()).adaptive_autorange().median * 1_000_000: 4.4f} us\"\n)\nblock_tds_c = torch.compile(block_tds, mode=\"reduce-overhead\")\nfor _ in range(5):  # warmup\n    block_tds_c(x=x)\nprint(\n    f\"Compiled sequential: {Timer('block_tds_c(x=x)', globals=globals()).adaptive_autorange().median * 1_000_000: 4.4f} us\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As one can see, the onverhead introduced by :class:`~tensordict.nn.TensorDictSequential` has been completely resolved.\n\n## Do's and don't with TensorDictModule\n\n- Don't use :class:`~torch.nn.Sequence` around modules from :mod:`tensordict.nn`. It would break the input/output\n  key structure.\n  Always try to rely on :class:`~tensordict.nn:TensorDictSequential` instead.\n\n- Don't assign the output tensordict to a new variable, as the output tensordict is just the input modified in-place.\n  Assigning a new variable name isn't strictly prohibited, but it means that you may wish for both of them to disappear\n  when one is deleted, when in fact the garbage collector will still see the tensors in the workspace and the no memory\n  will be freed:\n\n```\n>>> tensordict = module(tensordict)  # ok!\n>>> tensordict_out = module(tensordict)  # don't!\n```\n## Working with distributions: :class:`~tensordict.nn.ProbabilisticTensorDictModule`\n\n:class:`~tensordict.nn.ProbabilisticTensorDictModule` is a non-parametric module representing a\nprobability distribution. Distribution parameters are read from tensordict\ninput, and the output is written to an output tensordict. The output is\nsampled given some rule, specified by the input ``default_interaction_type``\nargument and the :func:`~tensordict.nn.interaction_type` global function. If they conflict,\nthe context manager precedes.\n\nIt can be wired together with a :class:`~tensordict.nn.TensorDictModule` that returns\na tensordict updated with the distribution parameters using\n:class:`~tensordict.nn.ProbabilisticTensorDictSequential`. This is a special case of\n:class:`~tensordict.nn.TensorDictSequential` whose last layer is a\n:class:`~tensordict.nn.ProbabilisticTensorDictModule` instance.\n\n:class:`~tensordict.nn.ProbabilisticTensorDictModule` is responsible for constructing the\ndistribution (through the :meth:`~tensordict.nn.ProbabilisticTensorDictModule.get_dist` method) and/or\nsampling from this distribution (through a regular `forward` call to the module). The same\n:meth:`~tensordict.nn.ProbabilisticTensorDictModule.get_dist` method is exposed within\n:class:`~tensordict.nn.ProbabilisticTensorDictSequential`.\n\nOne can find the parameters in the output tensordict as well as the log\nprobability if needed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensordict.nn import (\n    ProbabilisticTensorDictModule,\n    ProbabilisticTensorDictSequential,\n)\nfrom tensordict.nn.distributions import NormalParamExtractor\nfrom torch import distributions as dist\n\ntd = TensorDict({\"input\": torch.randn(3, 4), \"hidden\": torch.randn(3, 8)}, [3])\nnet = torch.nn.GRUCell(4, 8)\nnet = TensorDictModule(net, in_keys=[\"input\", \"hidden\"], out_keys=[\"hidden\"])\nextractor = NormalParamExtractor()\nextractor = TensorDictModule(extractor, in_keys=[\"hidden\"], out_keys=[\"loc\", \"scale\"])\ntd_module = ProbabilisticTensorDictSequential(\n    net,\n    extractor,\n    ProbabilisticTensorDictModule(\n        in_keys=[\"loc\", \"scale\"],\n        out_keys=[\"action\"],\n        distribution_class=dist.Normal,\n        return_log_prob=True,\n    ),\n)\nprint(f\"TensorDict before going through module: {td}\")\ntd_module(td)\nprint(f\"TensorDict after going through module now as keys action, loc and scale: {td}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nWe have seen how `tensordict.nn` can be used to dynamically build complex neural architectures on-the-fly.\nThis opens the possibility of building pipelines that are oblivious to the model signature, i.e., write generic codes\nthat use networks with an arbitrary number of inputs or outputs in a flexible manner.\n\nWe have also seen how :class:`~tensordict.nn.dispatch` enables to use `tensordict.nn` to build such networks and use\nthem without recurring to :class:`~tensordict.TensorDict` directly. Thanks to :func:`~torch.compile`, the overhead\nintroduced by :class:`tensordict.nn.TensorDictSequential` can be completely removed, leaving users with a neat,\ntensordict-free version of their module.\n\nIn the next tutorial, we will be seeing how ``torch.export`` can be used to isolate a module and export it.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}