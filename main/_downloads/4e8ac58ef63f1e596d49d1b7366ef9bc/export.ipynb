{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Exporting tensordict modules\n\n**Author**: [Vincent Moens](https://github.com/vmoens)\n\n## Prerequisites\n\nReading the `TensorDictModule <tensordictmodule>` tutorial is preferable to fully benefit from this tutorial.\n\nOnce a module has been written using ``tensordict.nn``, it is often useful to isolate the computational graph and export\nthat graph. The goal of this may be to execute the model on hardware (e.g., robots, drones, edge devices) or eliminate\nthe dependency on tensordict altogether.\n\nPyTorch provides multiple methods for exporting modules, including ``onnx`` and ``torch.export``, both of which are\ncompatible with ``tensordict``.\n\nIn this short tutorial, we will see how one can use ``torch.export`` to isolate the computational graph of a model.\n``torch.onnx`` support follows the same logic.\n\n## Key learnings\n\n- Executing a ``tensordict.nn`` module without :class:`~tensordict.TensorDict` inputs;\n- Selecting the output(s) of a model;\n- Exporting such model using `torch.export`;\n- Saving the model to a file;\n- Isolating the pytorch model;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\n\nimport torch\nfrom tensordict.nn import (\n    NormalParamExtractor,\n    TensorDictModule as Mod,\n    TensorDictSequential as Seq,\n)\nfrom torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Designing the model\n\nLet us build a simple neural network using ``tensordict.nn``. The network will consist of:\n\n- A linear layer mapping input to a hidden representation;\n- A ReLU activation;\n- A final linear layer producing the output.\n\nWe will also include a :class:`tensordict.nn.NormalParamExtractor` to demonstrate how to extract\nmultiple outputs from a single tensor.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = Seq(\n    # 1. A small network for embedding\n    Mod(nn.Linear(3, 4), in_keys=[\"x\"], out_keys=[\"hidden\"]),\n    Mod(nn.ReLU(), in_keys=[\"hidden\"], out_keys=[\"hidden\"]),\n    Mod(nn.Linear(4, 4), in_keys=[\"hidden\"], out_keys=[\"latent\"]),\n    # 2. Extracting params (splits into loc and scale)\n    Mod(NormalParamExtractor(), in_keys=[\"latent\"], out_keys=[\"loc\", \"scale\"]),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us run this model and see what the output looks like:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.randn(1, 3)\nprint(model(x=x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, running the model with a tensor input returns as many tensors as the module's output keys! For large\nmodels, this can be quite annoying and wasteful. Later, we will see how we can limit the number of outputs of the\nmodel to deal with this issue.\n\n### Using ``torch.export`` with a ``TensorDictModule``\n\nNow that we have successfully built our model, we would like to extract its computational graph in a single object that\nis independent of ``tensordict``. ``torch.export`` is a PyTorch module dedicated to isolating the graph of a module and\nrepresent it in a standardized way. Its main entry point is :func:`~torch.export.export` which returns an ``ExportedProgram``\nobject. In turn, this object has several attributes of interest that we will explore below: a ``graph_module``,\nwhich represents the FX graph captured by ``export``, a ``graph_signature`` with inputs, outputs, etc., of the graph,\nand finally a ``module()`` that returns a callable that can be used in-place of the original module.\n\nAlthough our module accepts both args and kwargs, we will focus on its usage with kwargs as this is clearer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.export import export\n\nmodel_export = export(model, args=(), kwargs={\"x\": x})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us look at the module:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"module:\", model_export.module())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This module can be run exactly like our original module (with a lower overhead):\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t0 = time.time()\nmodel(x=x)\nprint(f\"Time for TDModule: {(time.time() - t0) * 1e6: 4.2f} micro-seconds\")\nexported = model_export.module()\n\n# Exported version\nt0 = time.time()\nexported(x=x)\nprint(f\"Time for exported module: {(time.time() - t0) * 1e6: 4.2f} micro-seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and the FX graph:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"fx graph:\", model_export.graph_module.print_readable())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Working with nested keys\n\nNested keys are a core feature of the tensordict library, and being able to export modules that read and write\nnested entries is therefore an important feature to support.\nBecause keyword arguments must be regular strings, it is not possible for :class:`~tensordict.nn.dispatch` to work\ndirectly with them. Instead, ``dispatch`` will unpack nested keys joined with a regular underscore (`\"_\"`), as the\nfollowing example shows.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_nested = Seq(\n    Mod(lambda x: x + 1, in_keys=[(\"some\", \"key\")], out_keys=[\"hidden\"]),\n    Mod(lambda x: x - 1, in_keys=[\"hidden\"], out_keys=[(\"some\", \"output\")]),\n).select_out_keys((\"some\", \"output\"))\n\nmodel_nested_export = export(model_nested, args=(), kwargs={\"some_key\": x})\nprint(\"exported module with nested input:\", model_nested_export.module())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the callable returned by `module()` is a pure python callable that can be in turn compiled using\n:func:`~torch.compile`.\n\n## Saving the exported module\n\n``torch.export`` has its own serialization protocol, :func:`~torch.export.save` and :func:`~torch.export.load`.\nConventionally, the `\".pt2\"` extension is to be used:\n\n  >>> torch.export.save(model_export, \"model.pt2\")\n\n### Selecting the outputs\n\nRecall that the ``tensordict.nn`` is to keep every intermediate value in the output, unless the user specifically asks\nfor only a specific value. During training, this can be very useful: one can easily log intermediate values of the\ngraph, or use them for other purposes (e.g., reconstruct a distribution based on its saved parameters, rather than\nsaving the :class:`~torch.distributions.Distribution` object itself). One could also argue that, during training, the\nimpact on memory of registering intermediate values is negligible since they are part of the computational graph\nused by ``torch.autograd`` to compute the parameter gradients.\n\nDuring inference, though, we most likely are only interested in specific outputs of the model.\nBecause we want to extract the model for usages that are independent of the ``tensordict`` library, it makes sense to\nisolate the only output we desire.\nTo do this, we have several options:\n\n1. Build the :meth:`~tensordict.nn.TensorDictSequential` with the ``selected_out_keys`` keyword argument, which will\n   induce the selection of the desired entries during calls to the module;\n2. Using the :meth:`~tensordict.nn.TensorDictModule.select_out_keys` method, which will modify the ``out_keys``\n   attribute in-place (this can be reverted through :meth:`~tensordict.nn.TensorDictModule.reset_out_keys`).\n3. Wrap the existing instance in a :meth:`~tensordict.nn.TensorDictSequential` that will filter out the unwanted keys:\n\n    >>> module_filtered = Seq(module, selected_out_keys=[\"loc\"])\n\nLet us test the model after selecting its output keys.\nWhen an `x` input is provided, we expect our model to output a single tensor corresponding to the `\"loc\"` output:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.select_out_keys(\"loc\")\nprint(model(x=x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the output is now a single tensor.\nWe can create a new exported graph from this. Its computational graph should be simplified:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_export = export(model, args=(), kwargs={\"x\": x})\nprint(\"module:\", model_export.module())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is all you need to know to use ``torch.export``. Please refer to the\n[official documentation](https://pytorch.org/docs/stable/export) for more info.\n\n### Next steps and further reading\n\n- Check the ``torch.export`` tutorial, available [here](https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html)_;\n- ONNX support: check the [ONNX tutorials](https://pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html)\n  to learn more about this feature. Exporting to ONNX is very similar to `torch.export` explained here.\n- For deployment of PyTorch code on servers without python environment, check the\n  [AOTInductor](https://pytorch.org/docs/main/torch.compiler_aot_inductor.html) documentation.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}