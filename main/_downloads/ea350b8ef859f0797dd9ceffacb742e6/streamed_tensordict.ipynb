{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Building tensordicts from streams\n\n**Author**: [Vincent Moens](https://github.com/vmoens)\n\nIn many real-world applications, data is generated continuously and at varying frequencies.\n\nFor example, sensor readings from IoT devices, financial transactions, or social media updates can all produce streams\nof data that need to be processed and analyzed in real-time.\n\nWhen working with such data streams, it's often necessary to \"bucketize\" the incoming data into discrete chunks,\nallowing for efficient processing and analysis. However, this can be challenging when dealing with data streams that\nhave different frequencies or formats.\n\nIn this tutorial, we'll explore how to use TensorDict to build and manipulate data streams.\nWe'll learn how to create lazy stacks of tensors, handle asynchronous data streams, and densify our data for efficient\nstorage and processing.\n\nIn this tutorial, you will learn:\n- How to read streams of data and write them at regular intervals within a tensordict;\n- How to build TensorDict that stack contents with heterogeneous shapes together;\n- How to densify these tensors in single storages using ``nested_tensor`` if required.\n\n## Stacking heterogeneous tensordicts together\n\nIn many real-life scenarios, data come in streams that have different defined frequencies.\n\nOur goal in this tutorial is to \"bucketize\" the upcoming data such that it can be read and processed at a given\nslower frequency.\nThe challenge in this scenario is that the data may not be representable in regular \"rectangular\" format (i.e., where\neach dimension of the tensor is well-defined), but it could be the case that one bucket of data has more element than\nanother, in which case we cannot simply stack them together. Typically, consider the case where the first and second\nbuckets of data are as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom tensordict import TensorDict\n\nbucket0 = TensorDict(stream0=torch.randn(5), stream1=torch.randn(4))\nbucket1 = TensorDict(stream0=torch.randn(4), stream1=torch.randn(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In principle, we cannot stack these two tensordict contiguously in memory as the shape of the two streams differ.\nFortunately, TensorDict offers a tool to group instances with heterogeneous tensor shapes together:\n:class:`~tensordict.LazyStackedTensorDict`.\nTo create a lazy stack, one can just call :meth:`~tensordict.TensorDict.lazy_stack`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = TensorDict.lazy_stack([bucket0, bucket1], dim=0)\nprint(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The resulting data is just a representation of the two tensordicts as if they had been stacked together along\ndimension 0. :class:`~tensordict.LazyStackedTensorDict` supports most common operations of the\n:class:`~tensordict.TensorDictBase` class, here are some examples:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_select = data.select(\"stream0\")\ndata_plus_1 = data + 1\ndata_apply = data.apply(lambda x: x + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Moreover, indexing it will return the original data we used to create the stack\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert data[0] is bucket0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Still, in some instances, one could wish to have a contiguous representation of the underlying data.\nTo do this, :class:`~tensordict.TensorDictBase` offers a :meth:`~tensordict.TensorDictBase.densify` method that\nwill stack the tensors that can be stacked, and attempt to represent the rest as ``nested_tensor`` instances:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_cont = data.densify()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Asynchronous streams of data\n\nLet us now switch to a more concrete example, where we create a function that streams data (in this case, just\nintegers incremented by 1 at each iteration) at a given frequency.\n\nTo pass the data across threads, the function will use a queue received as input:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import asyncio\nfrom typing import List\n\n\nasync def generate_numbers(frequency: float, queue: asyncio.Queue) -> None:\n    i = 0\n    while True:\n        await asyncio.sleep(1 / frequency)\n        await queue.put(i)\n        i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``collect_data`` function reads the data from the queue for a given amount of time.\nAs soon as ``timeout`` has passed, the function returns:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "async def collect_data(queue: asyncio.Queue, timeout: float) -> List[int]:\n    values = []\n\n    # We create a nested `collect` async function in order to be able to stop it as\n    #  soon as timeout is passed (see wait_for below).\n    async def collect():\n        nonlocal values\n        while True:\n            value = await queue.get()\n            values.append(value)\n\n    task = asyncio.create_task(collect())\n    try:\n        await asyncio.wait_for(task, timeout=timeout)\n    except asyncio.TimeoutError:\n        task.cancel()\n    return values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``wait7hz`` function reads the data from the queue for a given amount of time.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "async def wait7hz() -> None:\n    queue = asyncio.Queue()\n    generate_task = asyncio.create_task(generate_numbers(7, queue))\n    collect_data_task = asyncio.create_task(collect_data(queue, timeout=1))\n    values = await collect_data_task\n    # The ``generate_task`` has not been terminated\n    generate_task.cancel()\n    print(values)\n\n\nasyncio.run(wait7hz())\n\nfrom typing import Callable, Dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now design a class that inherits from :class:`~tensordict.LazyStackedTensorDict` and reads data coming\nfrom different streams and registers them in separate tensordicts.\nA nice feature of :class:`~tensordict.LazyStackedTensorDict` is that it can be built incrementally too, such that\nwe can simply register the new data coming in by extending the lazy stack up until we have collected enough data.\nHere is an implementation of this ``StreamedTensorDict`` class:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensordict import LazyStackedTensorDict, NestedKey, TensorDictBase\n\n\nclass StreamedTensorDict(LazyStackedTensorDict):\n    \"\"\"A lazy stack class that can be built from a dictionary of streams.\"\"\"\n\n    @classmethod\n    async def from_streams(\n        cls,\n        streams: Dict[NestedKey, Callable],\n        timeout: float,\n        batch_size: int,\n        densify: bool = True,\n    ) -> TensorDictBase:\n        td = cls(stack_dim=0)\n\n        # We construct a queue for each stream\n        queues = [asyncio.Queue() for _ in range(len(streams))]\n        tasks = []\n        for stream, queue in zip(streams.values(), queues):\n            task = asyncio.create_task(stream(queue))\n            tasks.append(task)\n        for _ in range(batch_size):\n            values_tasks = []\n            for queue in queues:\n                values_task = asyncio.create_task(collect_data(queue, timeout))\n                values_tasks.append(values_task)\n            values = await asyncio.gather(*values_tasks)\n            td.append(TensorDict(dict(zip(streams.keys(), values))))\n\n        # Cancel the generator tasks\n        for task in tasks:\n            task.cancel()\n        if densify:\n            return td.densify(layout=torch.strided)\n        return td"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, the ``main`` function will compose the streaming functions ``stream0`` and ``stream1`` and pass them to the\n``StreamedTensorDict.from_streams`` method which will collect ``batch_size`` batches of data for ``timeout=1`` second\neach:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "async def main() -> TensorDictBase:\n    def stream0(queue):\n        return generate_numbers(frequency=7, queue=queue)\n\n    def stream1(queue):\n        return generate_numbers(frequency=3, queue=queue)\n\n    # Running this should take about 10 seconds\n    return await StreamedTensorDict.from_streams(\n        {\"bucket0\": stream0, \"bucket1\": stream1}, timeout=1, batch_size=10\n    )\n\n\ntd = asyncio.run(main())\n\nprint(\"TensorDict from stream\", td)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's represent the data from both streams - should be equal to torch.arange() for batch_size * timeout * Hz\n <=> 1 * 10 secs * 3 or 7\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"bucket0 (7Hz, around 70 values)\", td[\"bucket0\"].values())\nprint(\"bucket1 (3Hz, around 30 values)\", td[\"bucket1\"].values())\nprint(\"shapes of bucket0 (7Hz, around 70 values)\", td[\"bucket0\"]._nested_tensor_size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIn this tutorial, we've explored the basics of working with TensorDict and asynchronous data streams.\nWe've learned how to create lazy stacks of tensors, handle asynchronous data streams using asyncio, and densify our\ndata for efficient storage and processing.\n\nWe've also seen how :class:`~tensordict.TensorDict` and :class:`~tensordict.LazyStackedTensorDict` can be used to\nsimplify complex data processing tasks, such as bucketizing data streams with different frequencies.\nBy leveraging the power of TensorDict and asyncio, you can build scalable and efficient data processing pipelines\nthat can handle even the most demanding real-world applications.\n\nThanks for following along with this tutorial! We hope you've found it helpful and informative.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}