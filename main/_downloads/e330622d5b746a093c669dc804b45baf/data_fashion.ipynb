{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Using TensorDict for datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial we demonstrate how ``TensorDict`` can be used to\nefficiently and transparently load and manage data inside a training\npipeline. The tutorial is based heavily on the [PyTorch Quickstart\nTutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)_,\nbut modified to demonstrate use of ``TensorDict``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\n\nfrom tensordict import MemoryMappedTensor, TensorDict\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``torchvision.datasets`` module contains a number of convenient pre-prepared\ndatasets. In this tutorial we'll use the relatively simple FashionMNIST dataset. Each\nimage is an item of clothing, the objective is to classify the type of clothing in\nthe image (e.g. \"Bag\", \"Sneaker\" etc.).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "training_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n)\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor(),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will create two tensordicts, one each for the training and test data. We create\nmemory-mapped tensors to hold the data. This will allow us to efficiently load\nbatches of transformed data from disk rather than repeatedly load and transform\nindividual images.\n\nFirst we create the :class:`~tensordict.MemoryMappedTensor` containers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "training_data_td = TensorDict(\n    {\n        \"images\": MemoryMappedTensor.empty(\n            (len(training_data), *training_data[0][0].squeeze().shape),\n            dtype=torch.float32,\n        ),\n        \"targets\": MemoryMappedTensor.empty((len(training_data),), dtype=torch.int64),\n    },\n    batch_size=[len(training_data)],\n    device=device,\n)\ntest_data_td = TensorDict(\n    {\n        \"images\": MemoryMappedTensor.empty(\n            (len(test_data), *test_data[0][0].squeeze().shape), dtype=torch.float32\n        ),\n        \"targets\": MemoryMappedTensor.empty((len(test_data),), dtype=torch.int64),\n    },\n    batch_size=[len(test_data)],\n    device=device,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we can iterate over the data to populate the memory-mapped tensors. This takes a\nbit of time, but performing the transforms up-front will save repeated effort during\ntraining later.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i, (img, label) in enumerate(training_data):\n    training_data_td[i] = TensorDict({\"images\": img, \"targets\": label}, [])\n\nfor i, (img, label) in enumerate(test_data):\n    test_data_td[i] = TensorDict({\"images\": img, \"targets\": label}, [])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DataLoaders\n\nWe'll create DataLoaders from the ``torchvision``-provided Datasets, as well as from\nour memory-mapped TensorDicts.\n\nSince ``TensorDict`` implements ``__len__`` and ``__getitem__`` (and also\n``__getitems__``) we can use it like a map-style Dataset and create a ``DataLoader``\ndirectly from it. Note that because ``TensorDict`` can already handle batched indices,\nthere is no need for collation, so we pass the identity function as ``collate_fn``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)  # noqa: TOR401\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)  # noqa: TOR401\n\ntrain_dataloader_td = DataLoader(  # noqa: TOR401\n    training_data_td, batch_size=batch_size, collate_fn=lambda x: x\n)\ntest_dataloader_td = DataLoader(  # noqa: TOR401\n    test_data_td, batch_size=batch_size, collate_fn=lambda x: x\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model\n\nWe use the same model from the\n[Quickstart Tutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)_.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28 * 28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nmodel = Net().to(device)\nmodel_td = Net().to(device)\nmodel, model_td"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizing the parameters\n\nWe'll optimise the parameters of the model using stochastic gradient descent and\ncross-entropy loss.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\noptimizer_td = torch.optim.SGD(model_td.parameters(), lr=1e-3)\n\n\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training loop for our ``TensorDict``-based DataLoader is very similar, we just\nadjust how we unpack the data to the more explicit key-based retrieval offered by\n``TensorDict``. The ``.contiguous()`` method loads the data stored in the memmap tensor.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_td(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n\n    for batch, data in enumerate(dataloader):\n        X, y = data[\"images\"].contiguous(), data[\"targets\"].contiguous()\n\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n\n\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n\n            pred = model(X)\n\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n\n    print(\n        f\"Test Error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n    )\n\n\ndef test_td(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for batch in dataloader:\n            X, y = batch[\"images\"].contiguous(), batch[\"targets\"].contiguous()\n\n            pred = model(X)\n\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n\n    print(\n        f\"Test Error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n    )\n\n\nfor d in train_dataloader_td:\n    print(d)\n    break\n\nimport time\n\nt0 = time.time()\nepochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t + 1}\\n-------------------------\")\n    train_td(train_dataloader_td, model_td, loss_fn, optimizer_td)\n    test_td(test_dataloader_td, model_td, loss_fn)\nprint(f\"TensorDict training done! time: {time.time() - t0: 4.4f} s\")\n\nt0 = time.time()\nepochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t + 1}\\n-------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\nprint(f\"Training done! time: {time.time() - t0: 4.4f} s\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}