{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Simplifying PyTorch Memory Management with TensorDict\n**Author**: [Tom Begley](https://github.com/tcbegley)\n\nIn this tutorial you will learn how to control where the contents of a\n:class:`TensorDict` are stored in memory, either by sending those contents to a device,\nor by utilizing memory maps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Devices\nWhen you create a :class:`TensorDict`, you can specify a device with the ``device``\nkeyword argument. If the ``device`` is set, then all entries of the\n:class:`TensorDict` will be placed on that device. If the ``device`` is not set, then\nthere is no requirement that entries in the :class:`TensorDict` must be on the same\ndevice.\n\nIn this example we instantiate a :class:`TensorDict` with ``device=\"cuda:0\"``. When\nwe print the contents we can see that they have been moved onto the device.\n\n```\n>>> import torch\n>>> from tensordict import TensorDict\n>>> tensordict = TensorDict({\"a\": torch.rand(10)}, [10], device=\"cuda:0\")\n>>> print(tensordict)\nTensorDict(\n    fields={\n        a: Tensor(shape=torch.Size([10]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n    batch_size=torch.Size([10]),\n    device=cuda:0,\n    is_shared=True)\n```\nIf the device of the :class:`TensorDict` is not ``None``, new entries are also moved\nonto the device.\n\n```\n>>> tensordict[\"b\"] = torch.rand(10, 10)\n>>> print(tensordict)\nTensorDict(\n    fields={\n        a: Tensor(shape=torch.Size([10]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        b: Tensor(shape=torch.Size([10, 10]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n    batch_size=torch.Size([10]),\n    device=cuda:0,\n    is_shared=True)\n```\nYou can check the current device of the :class:`TensorDict` with the ``device``\nattribute.\n\n```\n>>> print(tensordict.device)\ncuda:0\n```\nThe contents of the :class:`TensorDict` can be sent to a device like a PyTorch tensor\nwith :meth:`TensorDict.cuda() <tensordict.TensorDict.cuda>` or\n:meth:`TensorDict.device(device) <tensordict.TensorDict.device>` with ``device``\nbeing the desired device.\n\n```\n>>> tensordict.to(torch.device(\"cpu\"))\n>>> print(tensordict)\nTensorDict(\n    fields={\n        a: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.float32, is_shared=False),\n        b: Tensor(shape=torch.Size([10, 10]), device=cpu, dtype=torch.float32, is_shared=False)},\n    batch_size=torch.Size([10]),\n    device=cpu,\n    is_shared=False)\n>>> tensordict.cuda()\n>>> print(tensordict)\nTensorDict(\n    fields={\n        a: Tensor(shape=torch.Size([10]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        b: Tensor(shape=torch.Size([10, 10]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n    batch_size=torch.Size([10]),\n    device=cuda:0,\n    is_shared=True)\n```\nThe :meth:`TensorDict.device <tensordict.TensorDict.device>` method requires a valid\ndevice to be passed as the argument. If you want to remove the device from the\n:class:`TensorDict` to allow values with different devices, you should use the\n:meth:`TensorDict.clear_device <tensordict.TensorDict.clear_device>` method.\n\n```\n>>> tensordict.clear_device()\n>>> print(tensordict)\nTensorDict(\n    fields={\n        a: Tensor(shape=torch.Size([10]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        b: Tensor(shape=torch.Size([10, 10]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n    batch_size=torch.Size([10]),\n    device=None,\n    is_shared=False)\n```\n## Memory-mapped Tensors\n``tensordict`` provides a class :class:`~tensordict.MemoryMappedTensor`\nwhich allows us to store the contents of a tensor on disk, while still\nsupporting fast indexing and loading of the contents in batches.\nSee the [ImageNet Tutorial](./tensorclass_imagenet.html) for an\nexample of this in action.\n\nTo convert the :class:`TensorDict` to a collection of memory-mapped tensors, use the\n:meth:`TensorDict.memmap_ <tensordict.TensorDict.memmap_>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict({\"a\": torch.rand(10), \"b\": {\"c\": torch.rand(10)}}, [10])\ntensordict.memmap_()\n\nprint(tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively one can use the\n:meth:`TensorDict.memmap_like <tensordict.TensorDict.memmap_like>` method. This will\ncreate a new :class:`~.TensorDict` of the same structure with\n:class:`~tensordict.MemoryMappedTensor` values, however it will not copy the\ncontents of the original tensors to the\nmemory-mapped tensors. This allows you to create the memory-mapped\n:class:`~.TensorDict` and then populate it slowly, and hence should generally be\npreferred to ``memmap_``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict({\"a\": torch.rand(10), \"b\": {\"c\": torch.rand(10)}}, [10])\nmm_tensordict = tensordict.memmap_like()\n\nprint(mm_tensordict[\"a\"].contiguous())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default the contents of the :class:`TensorDict` will be saved to a temporary\nlocation on disk, however if you would like to control where they are saved you can\nuse the keyword argument ``prefix=\"/path/to/root\"``.\n\nThe contents of the :class:`TensorDict` are saved in a directory structure that mimics\nthe structure of the :class:`TensorDict` itself. The contents of the tensor is saved\nin a NumPy memmap, and the metadata in an associated PyTorch save file. For example,\nthe above :class:`TensorDict` is saved as follows:\n\n::\n\n   \u251c\u2500\u2500 a.memmap\n   \u251c\u2500\u2500 a.meta.pt\n   \u251c\u2500\u2500 b\n   \u2502 \u251c\u2500\u2500 c.memmap\n   \u2502 \u251c\u2500\u2500 c.meta.pt\n   \u2502 \u2514\u2500\u2500 meta.pt\n   \u2514\u2500\u2500 meta.pt\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}